{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6794ab44-0b13-44ad-b332-68a41970b56b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in ./.venv/lib/python3.13/site-packages (25.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6fec2ec-3ebf-4ed9-911d-7e15e12a6236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad4b9acc-8621-4075-9c2f-07c275148162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp310-cp310-macosx_11_0_arm64.whl.metadata (91 kB)\n",
      "Requirement already satisfied: numpy>=1.22.4 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading pandas-2.3.1-cp310-cp310-macosx_11_0_arm64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Installing collected packages: pytz, tzdata, pandas\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3/3\u001b[0m [pandas]2m2/3\u001b[0m [pandas]\n",
      "\u001b[1A\u001b[2KSuccessfully installed pandas-2.3.1 pytz-2025.2 tzdata-2025.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6bd9af8a-478d-480f-99a6-1f8d3cd9262f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (2.19.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (2.3.1)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (4.25.8)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (2.32.4)\n",
      "Requirement already satisfied: setuptools in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (80.9.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (4.14.1)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (1.73.1)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (1.26.4)\n",
      "Requirement already satisfied: h5py>=3.11.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (3.14.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorflow) (0.37.1)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorflow) (2025.7.9)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.8.2)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
      "Requirement already satisfied: optree in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from keras>=3.5.0->tensorflow) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3053fa4f-8bc4-4a2f-a127-99f0b55a549f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (4.12.0.88)\n",
      "Collecting numpy<2.3.0,>=2 (from opencv-python)\n",
      "  Using cached numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Using cached numpy-2.2.6-cp310-cp310-macosx_14_0_arm64.whl (5.3 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "mediapipe 0.10.21 requires numpy<2, but you have numpy 2.2.6 which is incompatible.\n",
      "tensorflow 2.19.0 requires numpy<2.2.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4383c8bc-4475-483f-8fba-577a716502ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from matplotlib) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from matplotlib) (2.2.6)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b9298fef-706b-412c-a07e-82d688df04ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (1.7.0)\n",
      "Requirement already satisfied: numpy>=1.22.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from scikit-learn) (2.2.6)\n",
      "Requirement already satisfied: scipy>=1.8.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2608e20-5afd-4e72-8892-a2343486d759",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting seaborn\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from seaborn) (2.2.6)\n",
      "Requirement already satisfied: pandas>=1.2 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from seaborn) (2.3.1)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from seaborn) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.5)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.13.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1db31421-231c-4d0e-b990-93f2ad92bcc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting streamlit\n",
      "  Using cached streamlit-1.48.0-py3-none-any.whl.metadata (9.5 kB)\n",
      "Collecting altair!=5.4.0,!=5.4.1,<6,>=4.0 (from streamlit)\n",
      "  Using cached altair-5.5.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting blinker<2,>=1.5.0 (from streamlit)\n",
      "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting cachetools<7,>=4.0 (from streamlit)\n",
      "  Using cached cachetools-6.1.0-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting click<9,>=7.0 (from streamlit)\n",
      "  Using cached click-8.2.1-py3-none-any.whl.metadata (2.5 kB)\n",
      "Requirement already satisfied: numpy<3,>=1.23 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from streamlit) (2.2.6)\n",
      "Requirement already satisfied: packaging<26,>=20 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from streamlit) (25.0)\n",
      "Requirement already satisfied: pandas<3,>=1.4.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from streamlit) (2.3.1)\n",
      "Requirement already satisfied: pillow<12,>=7.1.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from streamlit) (11.3.0)\n",
      "Requirement already satisfied: protobuf<7,>=3.20 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from streamlit) (4.25.8)\n",
      "Collecting pyarrow>=7.0 (from streamlit)\n",
      "  Downloading pyarrow-21.0.0-cp310-cp310-macosx_12_0_arm64.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: requests<3,>=2.27 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from streamlit) (2.32.4)\n",
      "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
      "  Using cached tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting toml<2,>=0.10.1 (from streamlit)\n",
      "  Using cached toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from streamlit) (4.14.1)\n",
      "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
      "  Using cached gitpython-3.1.45-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
      "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
      "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from streamlit) (6.5.1)\n",
      "Requirement already satisfied: jinja2 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.24.0)\n",
      "Collecting narwhals>=1.14.2 (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit)\n",
      "  Using cached narwhals-2.0.1-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
      "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from requests<3,>=2.27->streamlit) (2025.7.9)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.26.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/nazmussakibsheam/Dev/Thesis/10 2/mp-env/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
      "Using cached streamlit-1.48.0-py3-none-any.whl (9.9 MB)\n",
      "Using cached altair-5.5.0-py3-none-any.whl (731 kB)\n",
      "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
      "Using cached cachetools-6.1.0-py3-none-any.whl (11 kB)\n",
      "Using cached click-8.2.1-py3-none-any.whl (102 kB)\n",
      "Using cached gitpython-3.1.45-py3-none-any.whl (208 kB)\n",
      "Using cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
      "Using cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
      "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
      "Using cached tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
      "Using cached toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Using cached narwhals-2.0.1-py3-none-any.whl (385 kB)\n",
      "Downloading pyarrow-21.0.0-cp310-cp310-macosx_12_0_arm64.whl (31.2 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m31.2/31.2 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m  \u001b[33m0:00:07\u001b[0mm0:00:01\u001b[0m00:01\u001b[0m\n",
      "Installing collected packages: toml, tenacity, smmap, pyarrow, narwhals, click, cachetools, blinker, pydeck, gitdb, gitpython, altair, streamlit\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m13/13\u001b[0m [streamlit]13\u001b[0m [streamlit]\n",
      "\u001b[1A\u001b[2KSuccessfully installed altair-5.5.0 blinker-1.9.0 cachetools-6.1.0 click-8.2.1 gitdb-4.0.12 gitpython-3.1.45 narwhals-2.0.1 pyarrow-21.0.0 pydeck-0.9.1 smmap-5.0.2 streamlit-1.48.0 tenacity-9.1.2 toml-0.10.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1ec0284a-51cb-41e9-9254-befa25eb7751",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from collections import deque\n",
    "from PIL import Image, ImageDraw, ImageFont"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d93d2379-f067-4ae4-97f1-55ffe9cc9417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress Keras PyDataset warning\n",
    "warnings.filterwarnings('ignore', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7d95bfcc-dbb2-4f63-bd53-93f9e35ca89d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('face_expression_detector.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7fe546c2-ab11-4842-b78e-2e80d3550b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 16\n",
    "NUM_CLASSES = 7\n",
    "EMOTION_LABELS = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']\n",
    "# DATASET_PATH = \"/Users/nazmussakibsheam/GUB/Data-mining-lab/face_expression_detector/webcam_dataset\"\n",
    "DATASET_PATH = \"/Users/nazmussakibsheam/GUB/Data-mining-lab/face_expression_detector/fer2013\"\n",
    "TRAIN_PATH = os.path.join(DATASET_PATH, \"train\")\n",
    "TEST_PATH = os.path.join(DATASET_PATH, \"test\")\n",
    "MODEL_PATH = 'facial_expression_model.keras'\n",
    "EMOTION_HISTORY_PATH = 'emotion_history.csv'\n",
    "IMAGES_PER_EMOTION = 10\n",
    "TRAIN_SPLIT = 0.8\n",
    "EMOTION_EMOJIS = {\n",
    "    'Angry': 'üò£', 'Disgust': 'ü§¢', 'Fear': 'üò®', 'Happy': 'üòä',\n",
    "    'Sad': 'üò¢', 'Surprise': 'üòÆ', 'Neutral': 'üòê'\n",
    "}\n",
    "TREND_WINDOW = 50  # Number of recent predictions for trend graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "59f4f2e3-bd60-4697-b61d-066eafbeb1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_text_with_pil(frame, text, position, font_size=30, color=(0, 255, 0)):\n",
    "    \"\"\"Draw text with emojis on an OpenCV frame using PIL.\"\"\"\n",
    "    # Convert OpenCV frame (BGR) to PIL Image (RGB)\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    pil_img = Image.fromarray(frame_rgb)\n",
    "    draw = ImageDraw.Draw(pil_img)\n",
    "\n",
    "    # Try emoji-compatible fonts in order of preference\n",
    "    font = None\n",
    "    font_name = None\n",
    "    font_paths = [\n",
    "        (\"/System/Library/Fonts/Apple Color Emoji.ttc\", \"Apple Color Emoji\"),  # macOS\n",
    "        (\"NotoColorEmoji.ttf\", \"Noto Color Emoji\"),  # Custom-installed font\n",
    "        (\"/System/Library/Fonts/Supplemental/Arial Unicode.ttf\", \"Arial Unicode\"),  # macOS fallback\n",
    "        (\"arial.ttf\", \"Arial\"),  # Windows\n",
    "        (\"DejaVuSans.ttf\", \"DejaVu Sans\")  # Linux\n",
    "    ]\n",
    "    \n",
    "    for path, name in font_paths:\n",
    "        try:\n",
    "            font = ImageFont.truetype(path, font_size)\n",
    "            font_name = name\n",
    "            logger.info(f\"Using font: {font_name}\")\n",
    "            break\n",
    "        except IOError:\n",
    "            continue\n",
    "    \n",
    "    if font is None:\n",
    "        font = ImageFont.load_default()\n",
    "        font_name = \"Default\"\n",
    "        logger.warning(f\"No emoji-compatible font found. Using default font (emojis may not display).\")\n",
    "\n",
    "    # Draw text\n",
    "    draw.text(position, text, font=font, fill=color)\n",
    "    \n",
    "    # Convert back to OpenCV format (BGR)\n",
    "    frame_bgr = cv2.cvtColor(np.array(pil_img), cv2.COLOR_RGB2BGR)\n",
    "    return frame_bgr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d1e8d571-e099-416b-95a9-7f6f9df8457f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_emojis():\n",
    "    \"\"\"Test rendering of all emojis in EMOTION_EMOJIS on a blank frame.\"\"\"\n",
    "    logger.info(\"Testing emoji rendering\")\n",
    "    # Create a blank frame\n",
    "    frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
    "    frame.fill(255)  # White background\n",
    "    \n",
    "    # Draw each emotion and its emoji\n",
    "    y_offset = 50\n",
    "    for emotion, emoji in EMOTION_EMOJIS.items():\n",
    "        text = f\"{emotion}: {emoji}\"\n",
    "        frame = draw_text_with_pil(frame, text, (50, y_offset), font_size=30, color=(0, 0, 255))\n",
    "        y_offset += 40\n",
    "    \n",
    "    # Display the frame\n",
    "    cv2.imshow('Emoji Test', frame)\n",
    "    logger.info(\"Displaying emoji test window. Press any key to exit.\")\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()\n",
    "    logger.info(\"Emoji test completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "27aa29e4-564f-4641-8c77-598dfc537e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset_dirs(dataset_path):\n",
    "    \"\"\"Create directory structure for train and test sets.\"\"\"\n",
    "    for split in ['train', 'test']:\n",
    "        split_path = os.path.join(dataset_path, split)\n",
    "        os.makedirs(split_path, exist_ok=True)\n",
    "        for emotion in EMOTION_LABELS:\n",
    "            os.makedirs(os.path.join(split_path, emotion), exist_ok=True)\n",
    "    logger.info(f\"Created dataset directories at {dataset_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3d798466-5ccf-4964-9c76-49e4f2304d42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_training_data(dataset_path):\n",
    "    \"\"\"Capture 10 images per emotion from webcam with 1-second delay.\"\"\"\n",
    "    create_dataset_dirs(dataset_path)\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    if face_cascade.empty():\n",
    "        logger.error(\"Failed to load Haar Cascade classifier\")\n",
    "        raise ValueError(\"Failed to load Haar Cascade classifier\")\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        logger.error(\"Failed to open webcam\")\n",
    "        raise ValueError(\"Failed to open webcam\")\n",
    "\n",
    "    logger.info(f\"Starting webcam to capture {IMAGES_PER_EMOTION} images per emotion.\")\n",
    "    logger.info(\"Wait until 10 images are captured per emotion. Press 'q' only when prompted.\")\n",
    "\n",
    "    for emotion in EMOTION_LABELS:\n",
    "        logger.info(f\"Capturing images for: {emotion}\")\n",
    "        count = 0\n",
    "        train_count = 0\n",
    "        test_count = 0\n",
    "        while count < IMAGES_PER_EMOTION:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                logger.error(\"Failed to capture frame from webcam\")\n",
    "                break\n",
    "\n",
    "            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=3, minSize=(30, 30))\n",
    "\n",
    "            for (x, y, w, h) in faces:\n",
    "                face = gray[y:y+h, x:x+w]\n",
    "                face = cv2.resize(face, IMG_SIZE)\n",
    "                face_rgb = cv2.cvtColor(face, cv2.COLOR_GRAY2RGB)\n",
    "                cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "                # Use PIL to draw emoji text\n",
    "                frame = draw_text_with_pil(frame, f\"{emotion} ({count}/{IMAGES_PER_EMOTION}) {EMOTION_EMOJIS[emotion]}\",\n",
    "                                          (x, y-30), font_size=30, color=(0, 255, 0))\n",
    "                frame = draw_text_with_pil(frame, f\"Avatar: {EMOTION_EMOJIS[emotion]}\",\n",
    "                                          (10, 30), font_size=30, color=(255, 0, 0))\n",
    "\n",
    "                split = 'train' if count < int(IMAGES_PER_EMOTION * TRAIN_SPLIT) else 'test'\n",
    "                save_path = os.path.join(dataset_path, split, emotion, f\"{emotion}_{count}.jpg\")\n",
    "                cv2.imwrite(save_path, face_rgb)\n",
    "                logger.info(f\"Saved {save_path}\")\n",
    "                count += 1\n",
    "                if split == 'train':\n",
    "                    train_count += 1\n",
    "                else:\n",
    "                    test_count += 1\n",
    "                time.sleep(1)\n",
    "                break\n",
    "\n",
    "            cv2.imshow(f'Capture {emotion}', frame)\n",
    "            if count >= IMAGES_PER_EMOTION:\n",
    "                logger.info(f\"Captured {train_count} train and {test_count} test images for {emotion}. Press 'q' to continue.\")\n",
    "                while True:\n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "            elif cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                logger.warning(f\"Only captured {count} images for {emotion}. Continue anyway? (y/n)\")\n",
    "                if input().lower() != 'y':\n",
    "                    logger.error(f\"Aborted capture for {emotion}. Please recapture all emotions.\")\n",
    "                    cap.release()\n",
    "                    cv2.destroyAllWindows()\n",
    "                    raise ValueError(f\"Incomplete capture for {emotion}\")\n",
    "\n",
    "        if emotion == EMOTION_LABELS[-1] and count >= IMAGES_PER_EMOTION:\n",
    "            logger.info(\"Finished capturing for all emotions.\")\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "07e625d5-08e9-47e7-abb6-513351aa8f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_directory(path, require_images=True):\n",
    "    \"\"\"Check if directory contains images for all emotions.\"\"\"\n",
    "    if not os.path.exists(path):\n",
    "        if require_images:\n",
    "            logger.error(f\"Directory {path} does not exist\")\n",
    "            raise ValueError(f\"Directory {path} does not exist\")\n",
    "        return False\n",
    "    for emotion in EMOTION_LABELS:\n",
    "        emotion_path = os.path.join(path, emotion)\n",
    "        if not os.path.exists(emotion_path) or (require_images and not os.listdir(emotion_path)):\n",
    "            if require_images:\n",
    "                logger.error(f\"No images found in {emotion_path}\")\n",
    "                raise ValueError(f\"No images found in {emotion_path}\")\n",
    "            return False\n",
    "    logger.info(f\"Validated directory {path}\")\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3689ca37-0a12-4605-a373-0defd39e2db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path, test_path):\n",
    "    \"\"\"Load and preprocess data using ImageDataGenerator.\"\"\"\n",
    "    validate_directory(train_path, require_images=True)\n",
    "    has_test_data = validate_directory(test_path, require_images=False)\n",
    "\n",
    "    train_datagen = ImageDataGenerator(\n",
    "        rescale=1./255,\n",
    "        rotation_range=20,\n",
    "        zoom_range=0.15,\n",
    "        width_shift_range=0.15,\n",
    "        height_shift_range=0.15,\n",
    "        shear_range=0.2,\n",
    "        brightness_range=[0.8, 1.2],\n",
    "        horizontal_flip=True,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=IMG_SIZE,\n",
    "        color_mode='rgb',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        classes=EMOTION_LABELS,\n",
    "        subset='training'\n",
    "    )\n",
    "    val_generator = train_datagen.flow_from_directory(\n",
    "        train_path,\n",
    "        target_size=IMG_SIZE,\n",
    "        color_mode='rgb',\n",
    "        batch_size=BATCH_SIZE,\n",
    "        class_mode='categorical',\n",
    "        classes=EMOTION_LABELS,\n",
    "        subset='validation'\n",
    "    )\n",
    "    test_generator = None\n",
    "    if has_test_data:\n",
    "        test_generator = test_datagen.flow_from_directory(\n",
    "            test_path,\n",
    "            target_size=IMG_SIZE,\n",
    "            color_mode='rgb',\n",
    "            batch_size=BATCH_SIZE,\n",
    "            class_mode='categorical',\n",
    "            classes=EMOTION_LABELS\n",
    "        )\n",
    "    return train_generator, val_generator, test_generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "1638686a-71ae-432e-aa52-b5b3b97c9e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \"\"\"Build MobileNetV2-based model for emotion classification.\"\"\"\n",
    "    base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
    "    base_model.trainable = False\n",
    "\n",
    "    model = Sequential([\n",
    "        base_model,\n",
    "        GlobalAveragePooling2D(),\n",
    "        Dense(128, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(NUM_CLASSES, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    logger.info(\"Built MobileNetV2 model\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "604fcbe2-bb40-48f4-92c3-65a33164092c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_generator, val_generator):\n",
    "    \"\"\"Train the model with early stopping and learning rate reduction.\"\"\"\n",
    "    callbacks = [\n",
    "        EarlyStopping(patience=5, restore_best_weights=True, monitor='val_loss'),\n",
    "        ModelCheckpoint(MODEL_PATH, save_best_only=True, monitor='val_loss'),\n",
    "        ReduceLROnPlateau(factor=0.5, patience=3, min_lr=1e-6)\n",
    "    ]\n",
    "    history = model.fit(\n",
    "        train_generator,\n",
    "        epochs=3,\n",
    "        validation_data=val_generator,\n",
    "        callbacks=callbacks\n",
    "    )\n",
    "    logger.info(\"Model training completed\")\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "cd2ff93b-391c-445d-a14b-08da1c6534ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, val_generator, test_generator):\n",
    "    \"\"\"Evaluate model on validation and test sets.\"\"\"\n",
    "    val_loss, val_accuracy = model.evaluate(val_generator)\n",
    "    logger.info(f\"Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "    if test_generator:\n",
    "        test_loss, test_accuracy = model.evaluate(test_generator)\n",
    "        logger.info(f\"Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}\")\n",
    "    else:\n",
    "        logger.warning(\"No test data available for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f6409ceb-6451-4018-a06b-2286e08c9dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(model, val_generator):\n",
    "    \"\"\"Plot confusion matrix for validation set.\"\"\"\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for images, labels in val_generator:\n",
    "        preds = model.predict(images, verbose=0)\n",
    "        y_pred.extend(np.argmax(preds, axis=1))\n",
    "        y_true.extend(np.argmax(labels, axis=1))\n",
    "        if len(y_pred) >= val_generator.samples:\n",
    "            break\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=EMOTION_LABELS, yticklabels=EMOTION_LABELS)\n",
    "    plt.title('Confusion Matrix (Validation Set)')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.savefig('confusion_matrix.png')\n",
    "    plt.show()\n",
    "    logger.info(\"Generated and saved confusion matrix\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5821887c-2276-40ba-aca8-ebe2d458ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"Plot and save training and validation accuracy/loss.\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['accuracy'], label='Train Accuracy', color='#1f77b4')\n",
    "    plt.plot(history.history['val_accuracy'], label='Validation Accuracy', color='#ff7f0e')\n",
    "    plt.title('Model Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], label='Train Loss', color='#1f77b4')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss', color='#ff7f0e')\n",
    "    plt.title('Model Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.savefig('training_history.png')\n",
    "    plt.show()\n",
    "    logger.info(\"Plotted and saved training history\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c6bfcec8-95bb-4aca-8e7d-dccd58e53dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trend_graph(frame, trend_data, width, height):\n",
    "    \"\"\"Draw a real-time emotion trend graph on the frame.\"\"\"\n",
    "    graph_height = 100\n",
    "    graph_width = 200\n",
    "    graph_x, graph_y = width - graph_width - 10, height - graph_height - 10\n",
    "    cv2.rectangle(frame, (graph_x, graph_y), (graph_x + graph_width, graph_y + graph_height), (255, 255, 255), -1)\n",
    "    cv2.rectangle(frame, (graph_x, graph_y), (graph_x + graph_width, graph_y + graph_height), (0, 0, 0), 1)\n",
    "\n",
    "    if len(trend_data) > 1:\n",
    "        max_points = min(len(trend_data), graph_width // 2)\n",
    "        step = graph_width / max_points\n",
    "        emotion_counts = [EMOTION_LABELS.index(emo) for emo in list(trend_data)[-max_points:]]\n",
    "        for i in range(len(emotion_counts) - 1):\n",
    "            x1 = int(graph_x + i * step)\n",
    "            x2 = int(graph_x + (i + 1) * step)\n",
    "            y1 = int(graph_y + graph_height - (emotion_counts[i] / NUM_CLASSES) * graph_height)\n",
    "            y2 = int(graph_y + graph_height - (emotion_counts[i + 1] / NUM_CLASSES) * graph_height)\n",
    "            cv2.line(frame, (x1, y1), (x2, y2), (0, 0, 255), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "9f6c5f69-e36f-48ef-9842-472d9672ff95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_webcam(model, save_video=False):\n",
    "    \"\"\"Process webcam feed with emotion detection, intensity bar, and trend graph.\"\"\"\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    if face_cascade.empty():\n",
    "        logger.error(\"Failed to load Haar Cascade classifier\")\n",
    "        raise ValueError(\"Failed to load Haar Cascade classifier\")\n",
    "\n",
    "    cap = cv2.VideoCapture(0)\n",
    "    if not cap.isOpened():\n",
    "        logger.error(\"Failed to open webcam\")\n",
    "        raise ValueError(\"Failed to open webcam\")\n",
    "\n",
    "    if save_video:\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter('output.avi', fourcc, 20.0, (640, 480))\n",
    "        logger.info(\"Recording video to output.avi\")\n",
    "\n",
    "    emotion_history = []\n",
    "    trend_data = deque(maxlen=TREND_WINDOW)\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        height, width = frame.shape[:2]\n",
    "    else:\n",
    "        height, width = 480, 640\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            logger.error(\"Failed to capture frame from webcam\")\n",
    "            break\n",
    "\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.05, minNeighbors=3, minSize=(30, 30))\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            face = gray[y:y+h, x:x+w]\n",
    "            face = cv2.resize(face, IMG_SIZE)\n",
    "            face_rgb = cv2.cvtColor(face, cv2.COLOR_GRAY2RGB)\n",
    "            face_rgb = face_rgb.astype('float32') / 255.0\n",
    "            face_rgb = np.expand_dims(face_rgb, axis=0)\n",
    "\n",
    "            prediction = model.predict(face_rgb, verbose=0)[0]\n",
    "            emotion = EMOTION_LABELS[np.argmax(prediction)]\n",
    "            emotion_history.append((datetime.now(), emotion, prediction.tolist()))\n",
    "            trend_data.append(emotion)\n",
    "\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "            # Use PIL to draw emoji text\n",
    "            frame = draw_text_with_pil(frame, f\"{emotion} {EMOTION_EMOJIS[emotion]}\",\n",
    "                                      (x, y-30), font_size=30, color=(0, 255, 0))\n",
    "            frame = draw_text_with_pil(frame, f\"Avatar: {EMOTION_EMOJIS[emotion]}\",\n",
    "                                      (10, 30), font_size=30, color=(255, 0, 0))\n",
    "\n",
    "            # Draw intensity bar\n",
    "            bar_width = 100\n",
    "            bar_height = 10\n",
    "            y_offset = 50\n",
    "            for i, (emo, prob) in enumerate(zip(EMOTION_LABELS, prediction)):\n",
    "                width = int(prob * bar_width)\n",
    "                cv2.rectangle(frame, (10, y_offset + i*15), (10 + width, y_offset + i*15 + bar_height),\n",
    "                             (0, 255, 255), -1)\n",
    "                cv2.putText(frame, f\"{emo}: {prob:.2f}\", (120, y_offset + i*15 + 10),\n",
    "                           cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 1)\n",
    "\n",
    "        # Draw trend graph\n",
    "        plot_trend_graph(frame, trend_data, width, height)\n",
    "\n",
    "        cv2.imshow('Facial Expression Recognition', frame)\n",
    "        if save_video:\n",
    "            out.write(frame)\n",
    "\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    if save_video:\n",
    "        out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    logger.info(\"Stopped webcam feed\")\n",
    "\n",
    "    # Save emotion history\n",
    "    df = pd.DataFrame(emotion_history, columns=['Timestamp', 'Emotion', 'Probabilities'])\n",
    "    df.to_csv(EMOTION_HISTORY_PATH, index=False)\n",
    "    logger.info(f\"Saved emotion history to {EMOTION_HISTORY_PATH}\")\n",
    "    return emotion_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb486b5-3a26-4c2a-96e2-ad2ed5152cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 22:26:44,361 - INFO - Starting Facial Expression Recognition project\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test emoji rendering first? (y/n)\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-09 22:26:49,057 - INFO - Testing emoji rendering\n",
      "2025-08-09 22:26:49,069 - INFO - Using font: Arial Unicode\n",
      "2025-08-09 22:26:49,072 - INFO - Using font: Arial Unicode\n",
      "2025-08-09 22:26:49,075 - INFO - Using font: Arial Unicode\n",
      "2025-08-09 22:26:49,077 - INFO - Using font: Arial Unicode\n",
      "2025-08-09 22:26:49,080 - INFO - Using font: Arial Unicode\n",
      "2025-08-09 22:26:49,082 - INFO - Using font: Arial Unicode\n",
      "2025-08-09 22:26:49,086 - INFO - Using font: Arial Unicode\n",
      "2025-08-09 22:26:49,104 - INFO - Displaying emoji test window. Press any key to exit.\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    \"\"\"Main function to orchestrate data capture, training, and prediction.\"\"\"\n",
    "    logger.info(\"Starting Facial Expression Recognition project\")\n",
    "\n",
    "    # Test emoji rendering\n",
    "    print(\"Test emoji rendering first? (y/n)\")\n",
    "    if input().lower() == 'y':\n",
    "        test_emojis()\n",
    "\n",
    "    # Capture webcam data\n",
    "    print(\"Capture training data first? (y/n)\")\n",
    "    if input().lower() == 'y':\n",
    "        try:\n",
    "            capture_training_data(DATASET_PATH)\n",
    "        except ValueError as e:\n",
    "            logger.error(f\"Capture failed: {e}\")\n",
    "            return\n",
    "\n",
    "    # Load data\n",
    "    try:\n",
    "        train_generator, val_generator, test_generator = load_data(TRAIN_PATH, TEST_PATH)\n",
    "    except ValueError as e:\n",
    "        logger.error(f\"Data loading failed: {e}\")\n",
    "        return\n",
    "\n",
    "    # Load pre-trained model or build and train new model\n",
    "    print(\"Load pre-trained model? (y/n)\")\n",
    "    if input().lower() == 'y' and os.path.exists(MODEL_PATH):\n",
    "        try:\n",
    "            model = tf.keras.models.load_model(MODEL_PATH)\n",
    "            logger.info(f\"Loaded pre-trained model from {MODEL_PATH}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {e}\")\n",
    "            print(f\"Failed to load model: {e}. Training a new model instead.\")\n",
    "            model = build_model()\n",
    "            history = train_model(model, train_generator, val_generator)\n",
    "    else:\n",
    "        model = build_model()\n",
    "        history = train_model(model, train_generator, val_generator)\n",
    "\n",
    "    # Evaluate model\n",
    "    evaluate_model(model, val_generator, test_generator)\n",
    "\n",
    "    # Plot results\n",
    "    try:\n",
    "        plot_history(history)\n",
    "    except NameError:\n",
    "        logger.info(\"No training history available for pre-trained model. Skipping history plot.\")\n",
    "    plot_confusion_matrix(model, val_generator)\n",
    "\n",
    "    # Process live webcam feed\n",
    "    logger.info(\"Starting webcam feed for emotion detection. Press 'q' to quit.\")\n",
    "    print(\"Save video output? (y/n)\")\n",
    "    save_video = input().lower() == 'y'\n",
    "    try:\n",
    "        emotion_history = process_webcam(model, save_video)\n",
    "    except ValueError as e:\n",
    "        logger.error(f\"Webcam processing failed: {e}\")\n",
    "        return\n",
    "\n",
    "    logger.info(\"Run 'streamlit run dashboard.py' to view the emotion dashboard.\")\n",
    "    print(\"Run 'streamlit run dashboard.py' to view the emotion dashboard at http://localhost:8501\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee1058e-fbb3-4bf0-bede-67a72a0f713d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\n",
      "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8502\u001b[0m\n",
      "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://192.168.10.89:8502\u001b[0m\n",
      "\u001b[0m\n",
      "\u001b[31m‚îÄ‚îÄ\u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\u001b[31m‚îÄ‚îÄ\u001b[0m\n",
      "\u001b[31m \u001b[0m \u001b[2;33m/Users/nazmussakibsheam/Dev/Thesis/anaconda3/lib/python3.13/site-packages/streamlit/\u001b[0m \u001b[31m \u001b[0m\n",
      "\u001b[31m \u001b[0m \u001b[2;33mruntime/scriptrunner/\u001b[0m\u001b[1;33mexec_code.py\u001b[0m:\u001b[94m121\u001b[0m in \u001b[92mexec_func_with_error_handling\u001b[0m               \u001b[31m \u001b[0m\n",
      "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
      "\u001b[31m \u001b[0m \u001b[2;33m/Users/nazmussakibsheam/Dev/Thesis/anaconda3/lib/python3.13/site-packages/streamlit/\u001b[0m \u001b[31m \u001b[0m\n",
      "\u001b[31m \u001b[0m \u001b[2;33mruntime/scriptrunner/\u001b[0m\u001b[1;33mscript_runner.py\u001b[0m:\u001b[94m645\u001b[0m in \u001b[92mcode_to_exec\u001b[0m                            \u001b[31m \u001b[0m\n",
      "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
      "\u001b[31m \u001b[0m \u001b[2;33m/Users/nazmussakibsheam/GUB/Data-mining-lab/face_expression_detector/\u001b[0m\u001b[1;33mdashboard.py\u001b[0m:\u001b[94m18\u001b[0m \u001b[31m \u001b[0m\n",
      "\u001b[31m \u001b[0m \u001b[94m0\u001b[0m in \u001b[92m<module>\u001b[0m                                                                        \u001b[31m \u001b[0m\n",
      "\u001b[31m \u001b[0m                                                                                      \u001b[31m \u001b[0m\n",
      "\u001b[31m \u001b[0m   \u001b[2m177 \u001b[0m\u001b[2m  \u001b[0m},                                                                           \u001b[31m \u001b[0m\n",
      "\u001b[31m \u001b[0m   \u001b[2m178 \u001b[0m\u001b[2m  \u001b[0m{                                                                            \u001b[31m \u001b[0m\n",
      "\u001b[31m \u001b[0m   \u001b[2m179 \u001b[0m\u001b[2m   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mcell_type\u001b[0m\u001b[33m\"\u001b[0m: \u001b[33m\"\u001b[0m\u001b[33mcode\u001b[0m\u001b[33m\"\u001b[0m,                                                        \u001b[31m \u001b[0m\n",
      "\u001b[31m \u001b[0m \u001b[31m‚ù± \u001b[0m180 \u001b[2m   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mexecution_count\u001b[0m\u001b[33m\"\u001b[0m: \u001b[1;4mnull\u001b[0m,                                                    \u001b[31m \u001b[0m\n",
      "\u001b[31m \u001b[0m   \u001b[2m181 \u001b[0m\u001b[2m   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mid\u001b[0m\u001b[33m\"\u001b[0m: \u001b[33m\"\u001b[0m\u001b[33m862af816-ed28-4994-9655-5878b119f887\u001b[0m\u001b[33m\"\u001b[0m,                               \u001b[31m \u001b[0m\n",
      "\u001b[31m \u001b[0m   \u001b[2m182 \u001b[0m\u001b[2m   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33mmetadata\u001b[0m\u001b[33m\"\u001b[0m: {},                                                             \u001b[31m \u001b[0m\n",
      "\u001b[31m \u001b[0m   \u001b[2m183 \u001b[0m\u001b[2m   \u001b[0m\u001b[33m\"\u001b[0m\u001b[33moutputs\u001b[0m\u001b[33m\"\u001b[0m: [],                                                              \u001b[31m \u001b[0m\n",
      "\u001b[31m‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\u001b[0m\n",
      "\u001b[1;91mNameError: \u001b[0mname \u001b[32m'null'\u001b[0m is not defined\n"
     ]
    }
   ],
   "source": [
    "!streamlit run dashboard.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b90b9d3-c7f5-4b89-9d32-2218c94ae9ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10 (mp-env)",
   "language": "python",
   "name": "mp-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
